{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "ExecuteTimeLog": [
      {
        "duration": 1602,
        "start_time": "2021-08-04T16:51:33.965Z"
      },
      {
        "duration": 3472,
        "start_time": "2021-08-04T16:54:24.853Z"
      },
      {
        "duration": 2044,
        "start_time": "2021-08-04T16:54:32.620Z"
      },
      {
        "duration": 728,
        "start_time": "2021-08-04T16:55:04.491Z"
      },
      {
        "duration": 45,
        "start_time": "2021-08-04T16:55:21.480Z"
      },
      {
        "duration": 9,
        "start_time": "2021-08-04T17:00:57.930Z"
      },
      {
        "duration": 322,
        "start_time": "2021-08-04T20:39:39.864Z"
      },
      {
        "duration": 261,
        "start_time": "2021-08-04T20:40:14.508Z"
      },
      {
        "duration": 6,
        "start_time": "2021-08-04T20:40:32.206Z"
      },
      {
        "duration": 1592,
        "start_time": "2021-08-04T20:41:14.698Z"
      },
      {
        "duration": 938,
        "start_time": "2021-08-04T20:41:16.292Z"
      },
      {
        "duration": 57,
        "start_time": "2021-08-04T20:41:17.234Z"
      },
      {
        "duration": 10,
        "start_time": "2021-08-04T20:41:17.294Z"
      },
      {
        "duration": 1536,
        "start_time": "2021-08-04T20:43:05.017Z"
      },
      {
        "duration": 684,
        "start_time": "2021-08-04T20:43:06.556Z"
      },
      {
        "duration": 59,
        "start_time": "2021-08-04T20:43:07.246Z"
      },
      {
        "duration": 10,
        "start_time": "2021-08-04T20:43:07.308Z"
      },
      {
        "duration": 1599,
        "start_time": "2021-08-04T20:43:25.461Z"
      },
      {
        "duration": 763,
        "start_time": "2021-08-04T20:43:27.062Z"
      },
      {
        "duration": 70,
        "start_time": "2021-08-04T20:43:27.829Z"
      },
      {
        "duration": 12,
        "start_time": "2021-08-04T20:43:27.903Z"
      },
      {
        "duration": 2672,
        "start_time": "2021-08-04T20:43:27.918Z"
      },
      {
        "duration": 17,
        "start_time": "2021-08-04T20:43:30.592Z"
      },
      {
        "duration": 424,
        "start_time": "2021-08-04T20:43:30.612Z"
      },
      {
        "duration": 259,
        "start_time": "2021-08-04T20:45:22.370Z"
      },
      {
        "duration": 1532,
        "start_time": "2021-08-04T20:46:43.539Z"
      },
      {
        "duration": 702,
        "start_time": "2021-08-04T20:46:45.073Z"
      },
      {
        "duration": 45,
        "start_time": "2021-08-04T20:46:45.782Z"
      },
      {
        "duration": 9,
        "start_time": "2021-08-04T20:46:45.830Z"
      },
      {
        "duration": 3,
        "start_time": "2021-08-04T20:46:45.841Z"
      },
      {
        "duration": 5,
        "start_time": "2021-08-04T20:46:45.847Z"
      },
      {
        "duration": 5,
        "start_time": "2021-08-04T20:46:45.883Z"
      },
      {
        "duration": 1566,
        "start_time": "2021-08-04T20:46:45.891Z"
      },
      {
        "duration": 1548,
        "start_time": "2021-08-04T20:47:16.016Z"
      },
      {
        "duration": 690,
        "start_time": "2021-08-04T20:47:17.566Z"
      },
      {
        "duration": 55,
        "start_time": "2021-08-04T20:47:18.259Z"
      },
      {
        "duration": 9,
        "start_time": "2021-08-04T20:47:18.317Z"
      },
      {
        "duration": 3,
        "start_time": "2021-08-04T20:47:18.328Z"
      },
      {
        "duration": 7,
        "start_time": "2021-08-04T20:47:18.333Z"
      },
      {
        "duration": 40,
        "start_time": "2021-08-04T20:47:18.342Z"
      },
      {
        "duration": 1600,
        "start_time": "2021-08-04T20:51:30.083Z"
      },
      {
        "duration": 744,
        "start_time": "2021-08-04T20:51:31.687Z"
      },
      {
        "duration": 59,
        "start_time": "2021-08-04T20:51:32.433Z"
      },
      {
        "duration": 10,
        "start_time": "2021-08-04T20:51:32.494Z"
      },
      {
        "duration": 8,
        "start_time": "2021-08-04T20:51:32.506Z"
      },
      {
        "duration": 7,
        "start_time": "2021-08-04T20:51:32.516Z"
      },
      {
        "duration": 1337,
        "start_time": "2021-08-06T18:57:38.692Z"
      },
      {
        "duration": 6081,
        "start_time": "2021-08-06T18:57:40.031Z"
      },
      {
        "duration": 1899,
        "start_time": "2021-08-06T18:57:46.114Z"
      },
      {
        "duration": 42,
        "start_time": "2021-08-06T18:57:48.015Z"
      },
      {
        "duration": 11,
        "start_time": "2021-08-06T18:57:48.058Z"
      },
      {
        "duration": 6,
        "start_time": "2021-08-06T18:57:48.070Z"
      },
      {
        "duration": 4,
        "start_time": "2021-08-06T18:57:48.078Z"
      },
      {
        "duration": 8,
        "start_time": "2021-08-06T18:57:48.083Z"
      },
      {
        "duration": 6,
        "start_time": "2021-08-06T18:57:48.093Z"
      },
      {
        "duration": 1337,
        "start_time": "2021-08-06T18:57:48.101Z"
      },
      {
        "duration": 1174,
        "start_time": "2021-08-06T18:59:48.137Z"
      },
      {
        "duration": 4,
        "start_time": "2021-08-06T19:00:25.937Z"
      },
      {
        "duration": 1382,
        "start_time": "2021-08-06T19:00:26.835Z"
      },
      {
        "duration": 279,
        "start_time": "2021-08-06T19:01:14.317Z"
      },
      {
        "duration": 939,
        "start_time": "2021-08-06T19:01:19.977Z"
      },
      {
        "duration": 1456,
        "start_time": "2021-08-06T19:01:38.635Z"
      }
    ],
    "colab": {
      "name": "notebook (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Содержание",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "302.391px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmitriygorlov/Yandex.Practikum_Data_Science/blob/main/Module-03_04-Machine-learning-for-texts/project-12_text-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l02qJWJB0LV"
      },
      "source": [
        "# Проект для «Викишоп»"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbSp9nlB0LX"
      },
      "source": [
        "Нужно обучить модель классифицировать комментарии на позитивные и негативные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "-k-KC3c2gIls"
      },
      "source": [
        "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Библиотеки-и-первичное-прочтение-файла\" data-toc-modified-id=\"Библиотеки-и-первичное-прочтение-файла-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Библиотеки и первичное прочтение файла</a></span></li><li><span><a href=\"#Подготовка-признаков\" data-toc-modified-id=\"Подготовка-признаков-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Подготовка признаков</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Линейная-регрессия\" data-toc-modified-id=\"Линейная-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Линейная регрессия</a></span></li><li><span><a href=\"#LGBM\" data-toc-modified-id=\"LGBM-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>LGBM</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-SRwRTtB0LZ"
      },
      "source": [
        "## Подготовка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FSjJlIvB0La"
      },
      "source": [
        "### Библиотеки и первичное прочтение файла"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKOC44veB0Lb",
        "outputId": "f7f4b462-5167-4d13-93ef-e035959cc45a"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pymystem3 import Mystem\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.corpus import stopwords as nltk_stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(nltk_stopwords.words('english'))\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from lightgbm import LGBMClassifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hwoy_YAB0Lf"
      },
      "source": [
        "загрузили библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7zfZaOLe6yA",
        "outputId": "4ee1aaa5-af11-4d7b-b531-2438b2e88bea"
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-05 19:52:42--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.244, 5.45.205.243, 5.45.205.241, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.244|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-mskm903.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
            "--2021-09-05 19:52:42--  http://cache-mskm903.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving cache-mskm903.cdn.yandex.net (cache-mskm903.cdn.yandex.net)... 5.45.220.13, 2a02:6b8:0:2002::14\n",
            "Connecting to cache-mskm903.cdn.yandex.net (cache-mskm903.cdn.yandex.net)|5.45.220.13|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  9.91MB/s    in 1.6s    \n",
            "\n",
            "2021-09-05 19:52:45 (9.91 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n",
            "cp: cannot create regular file '/root/.local/bin/mystem': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a5v_3tMrJQl"
      },
      "source": [
        "**специально загрузил версию младше, со старшей вешался google colab на лемматизации**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxqmHfoXB0Lh"
      },
      "source": [
        "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFfdMLuKB0Lk"
      },
      "source": [
        "загрузили файл, посмотрим ближе"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "tDL1mtHKB0Ln",
        "outputId": "d3e95390-8362-4526-e24a-3ee8f8abaabb"
      },
      "source": [
        "display(df.head())\n",
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  toxic\n",
              "0  Explanation\\nWhy the edits made under my usern...      0\n",
              "1  D'aww! He matches this background colour I'm s...      0\n",
              "2  Hey man, I'm really not trying to edit war. It...      0\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
              "4  You, sir, are my hero. Any chance you remember...      0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   text    159571 non-null  object\n",
            " 1   toxic   159571 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 2.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "e91hO5I_7qH1",
        "outputId": "70091ee2-cf36-4334-d5dc-1ee8f57020df"
      },
      "source": [
        "df.sample(n = 5, random_state = 12345)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146790</th>\n",
              "      <td>Ahh shut the fuck up you douchebag sand nigger...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2941</th>\n",
              "      <td>\"\\n\\nREPLY: There is no such thing as Texas Co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115087</th>\n",
              "      <td>Reply\\nHey, you could at least mention Jasenov...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48830</th>\n",
              "      <td>Thats fine, there is no deadline )   chi?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136034</th>\n",
              "      <td>\"\\n\\nDYK nomination of Mustarabim\\n Hello! You...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  toxic\n",
              "146790  Ahh shut the fuck up you douchebag sand nigger...      1\n",
              "2941    \"\\n\\nREPLY: There is no such thing as Texas Co...      0\n",
              "115087  Reply\\nHey, you could at least mention Jasenov...      0\n",
              "48830           Thats fine, there is no deadline )   chi?      0\n",
              "136034  \"\\n\\nDYK nomination of Mustarabim\\n Hello! You...      0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTfKlktoB0Lo",
        "outputId": "0f2f7eca-ae8d-4da3-a05c-98743f6295a8"
      },
      "source": [
        "df['toxic'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    143346\n",
              "1     16225\n",
              "Name: toxic, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfHU0WB3B0Lp"
      },
      "source": [
        "Файл достаточно большой, содержит почти 160 000 строк, важно учитывать при тяжких задачах. При этом у нас классическая задача классификации текста. Есть признак text, который необходимо подготовить, а также у нас есть целевой признак toxic с большим перекосом значений (9 к 1), что может быть проблемой для достижения заданной f-меры. Для начала разберёмся со столбцом text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlYKqbtTB0Lq"
      },
      "source": [
        "### Подготовка признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvfspRdAB0Lq"
      },
      "source": [
        "Подготовим функции и создадим новый столбец"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJRK5vWbB0Ls"
      },
      "source": [
        "# df['text'] = df['text'].values.astype('U')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee0UHQwhXaqw"
      },
      "source": [
        "# m = Mystem()\n",
        "\n",
        "# def clear_lemm_text(text):\n",
        "#     filtro = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "#     splito = filtro.split()\n",
        "#     clearo = ' '.join(splito)\n",
        "#     lemm_list = m.lemmatize(clearo)\n",
        "#     lemm_text = ' '.join(lemm_list)\n",
        "#     return lemm_text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUK_YNraDiQ9"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def clear_lemm_text_word(text):\n",
        "    filtro = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
        "    splito = filtro.split()\n",
        "    # clearo = ' '.join(splito)\n",
        "    lemm_list = list()\n",
        "    for w in nltk.word_tokenize(' '.join(splito)):\n",
        "        lemmm_uno = lemmatizer.lemmatize(w, get_wordnet_pos(w))\n",
        "        lemm_list.append(lemmm_uno)\n",
        "    lemm_text = ' '.join(lemm_list)\n",
        "    return lemm_text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL1EuqBrnqu8",
        "outputId": "96a1d974-e2c8-4032-d252-58181b075f2e"
      },
      "source": [
        "clifff = df['text'].head().apply(clear_lemm_text_word)\n",
        "print(clifff)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    Explanation Why the edits make under my userna...\n",
            "1    D aww He match this background colour I m seem...\n",
            "2    Hey man I m really not try to edit war It s ju...\n",
            "3    More I can t make any real suggestion on impro...\n",
            "4    You sir be my hero Any chance you remember wha...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F8CtcB6abNF"
      },
      "source": [
        "# df['text'] = df['text'].astype(str)\n",
        "df['ready_word_text'] = df['text'].apply(clear_lemm_text_word)\n",
        "# df['text'].head().apply(clear_lemm_text_word)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VArQKhX5Gk_l",
        "outputId": "3e66af71-1c00-42de-acb6-7c8d5ed664ba"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>ready_word_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits make under my userna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>D aww He match this background colour I m seem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>Hey man I m really not try to edit war It s ju...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>More I can t make any real suggestion on impro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>You sir be my hero Any chance you remember wha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                    ready_word_text\n",
              "0  Explanation\\nWhy the edits made under my usern...  ...  Explanation Why the edits make under my userna...\n",
              "1  D'aww! He matches this background colour I'm s...  ...  D aww He match this background colour I m seem...\n",
              "2  Hey man, I'm really not trying to edit war. It...  ...  Hey man I m really not try to edit war It s ju...\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...  ...  More I can t make any real suggestion on impro...\n",
              "4  You, sir, are my hero. Any chance you remember...  ...  You sir be my hero Any chance you remember wha...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRKNwuWaGk_m"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('averaged_perceptron_tagger')z"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZU68muLB0Lv"
      },
      "source": [
        "теперь пройдёмся по столбцу и создадим новый (очищенный и лемматизированны), готовый к использованию"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXbS7ax-B0Lw"
      },
      "source": [
        "# df['ready_text'] = df['text'].apply(clear_lemm_text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spq4vfdWB0Lw"
      },
      "source": [
        "теперь поделим на обучающую и тестовую части"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bshZLDyB0Lx"
      },
      "source": [
        "features = df['ready_word_text']\n",
        "target = df['toxic']\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features, target, test_size = 0.25, random_state = 12345, stratify = target)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GATiGIQYB0Lx"
      },
      "source": [
        "подготовка готова, перейдём к обучению"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8zr10njB0Ly"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDSQ6XLDB0Ly"
      },
      "source": [
        "Воспользуемся 2 моделями:\n",
        "\n",
        "- Линейная регрессия\n",
        "- LGBM\n",
        "\n",
        "(Тут было двухчасовое мучение с Catboost и google colab, но сражение было проиграно)\n",
        "\n",
        "Для начала создадим признаки с помощью TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V74d1DpFB0Lz",
        "outputId": "38c226b7-5123-4a28-ab47-0e4a896274f3"
      },
      "source": [
        "%%time\n",
        "count_tf_idf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1, 2)) \n",
        "\n",
        "tf_idf_train = count_tf_idf.fit_transform(features_train)\n",
        "tf_idf_test = count_tf_idf.transform(features_test)\n",
        "\n",
        "print('Размер train:', tf_idf_train.shape)\n",
        "print('Размер test:', tf_idf_test.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер train: (119678, 2019902)\n",
            "Размер test: (39893, 2019902)\n",
            "CPU times: user 28.6 s, sys: 659 ms, total: 29.3 s\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L6P9Y7fr4l3"
      },
      "source": [
        "Признаки созданы, перейдём к моделям\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYbH8z2SDjPW"
      },
      "source": [
        "### Линейная регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LJUHQkNxxlk"
      },
      "source": [
        "Для начала посмотрим на необходимость балансирования весов классов. Запустим 2 версии модели "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hApxrWakDuZp",
        "outputId": "536105d7-1db3-45a5-8f50-9536b3984674"
      },
      "source": [
        "%%time\n",
        "\n",
        "model_LR = LogisticRegression()\n",
        "train_f1 = cross_val_score(model_LR, \n",
        "                      tf_idf_train, \n",
        "                      target_train, \n",
        "                      cv=3, \n",
        "                      scoring='f1').mean()\n",
        "print('F1 на CV', train_f1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 на CV 0.5874094315963835\n",
            "CPU times: user 1min 20s, sys: 42.3 s, total: 2min 2s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qlQcNztGT2L",
        "outputId": "d960752b-0047-4ef6-dbc3-a297bdbf494e"
      },
      "source": [
        "%%time\n",
        "\n",
        "model_LR_balanced = LogisticRegression(class_weight='balanced')\n",
        "train_f1 = cross_val_score(model_LR_balanced, \n",
        "                      tf_idf_train, \n",
        "                      target_train, \n",
        "                      cv=3, \n",
        "                      scoring='f1').mean()\n",
        "print('F1 на CV', train_f1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 на CV 0.7500256181840493\n",
            "CPU times: user 1min 10s, sys: 37.8 s, total: 1min 48s\n",
            "Wall time: 1min 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiOdMGeiyCxb"
      },
      "source": [
        "Невооруженным взглядом видно, насколько баланс важен. Будем использовать только эту модель. На основе неё запустим перебор гиперпараметров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enRYOXy1ygOL",
        "outputId": "e49cfdfb-bce6-4a30-c7a8-5ec880b30923"
      },
      "source": [
        "%%time\n",
        "\n",
        "lr = LogisticRegression(max_iter = 1000, class_weight='balanced')\n",
        "lr_params = {'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "             'C':[0.1, 1, 10]}\n",
        "\n",
        "clf_lr = GridSearchCV(estimator = lr, param_grid = lr_params, scoring='f1', cv = 3)\n",
        "\n",
        "clf_lr.fit(tf_idf_train, target_train)\n",
        "\n",
        "print(clf_lr.best_params_)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 10, 'solver': 'newton-cg'}\n",
            "CPU times: user 11min 41s, sys: 6min 24s, total: 18min 5s\n",
            "Wall time: 10min 15s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiNG2coj-jrY"
      },
      "source": [
        "Лучшие параметры найдены, посмотрим на получившийся F1 на тестовой выборке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L0hM0NBGURi",
        "outputId": "f1f36369-73ec-41df-e170-beef74b94513"
      },
      "source": [
        "# clf_lr.fit(tf_idf_train, target_train)\n",
        "\n",
        "lr_predict = clf_lr.predict(tf_idf_test)\n",
        "\n",
        "print('F1 =', f1_score(target_test, lr_predict))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 = 0.7876344086021505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KarGY8EvB0L0"
      },
      "source": [
        "Несмотря на долгий подбор, модель Линейной регрессии справилась с заданным порогом метрики f1 в 0.75, показав результат в 0.787"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzZD4XDK7fJF"
      },
      "source": [
        "### LGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTQojfMm7t3P"
      },
      "source": [
        "Теперь попробуем LGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-CtNI1f7x7Y",
        "outputId": "c4d220b5-af26-480c-bb9d-36f3e279240d"
      },
      "source": [
        "%%time\n",
        "LGBM = LGBMClassifier(class_weight='balanced')\n",
        "parametrs_LGBM = {'n_estimators' : [100],\n",
        "              'depth' : [15, 5],\n",
        "              'learning_rate' : [0.2],\n",
        "              'random_state' : [12345]}\n",
        "clf_LGBM = GridSearchCV(estimator = LGBM, param_grid = parametrs_LGBM, scoring='f1', cv = 3)\n",
        "\n",
        "clf_LGBM.fit(tf_idf_train, target_train)\n",
        "\n",
        "print(clf_LGBM.best_params_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'depth': 15, 'learning_rate': 0.2, 'n_estimators': 100, 'random_state': 12345}\n",
            "CPU times: user 16min 2s, sys: 3.25 s, total: 16min 6s\n",
            "Wall time: 8min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4umXk1MzI9zt"
      },
      "source": [
        "Лучшие параметры найдены, посмотрим какую метрику покаже модель на тесте."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyif9lVlGHgU",
        "outputId": "a5d8217e-f24d-474b-a84a-931697db5c12"
      },
      "source": [
        "lgbm3 = LGBMClassifier(n_estimators = 100,\n",
        "              depth = 15,\n",
        "            learning_rate = 0.2,\n",
        "              random_state = 12345)\n",
        "\n",
        "lgbm3.fit(tf_idf_train, target_train)\n",
        "\n",
        "LGBM3_predict = lgbm3.predict(tf_idf_test)\n",
        "\n",
        "print('F1 =', f1_score(target_test, LGBM3_predict))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 = 0.7698478561549101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OL-SzDzJLfZ"
      },
      "source": [
        "LGBM показал отличный результат по F1 в 0.769, что также побило требуемый уровень в 0.75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-KehbVB0L0"
      },
      "source": [
        "## Выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWq9MVcnJZ4q"
      },
      "source": [
        "**По результатам исследования подобрана лучшая модель - это модель Линейной Регрессии с гиперпараметрами: max_iter = 1000, class_weight='balanced', C = 10, solver = lbfgs! . Она показала F1 выше требуемого (0.787)**\n",
        "\n",
        "*Важный комментарий: в целевом признаке наблюдается сильный дисбаланс, положительных признаков (токсичных комментариев) всего лишь 10% от общего числа! В связи с этим проверка на адекватность не проводилась, требуемый показатель F1 достаточно определяет целесообразность модели*\n",
        "\n",
        "В процессе исследования мы провели лемматизацию и очищение текстов, а также провели создание признаков с помощью TF-IDF, после чего протестировали 2 модели с разными гиперпараметрами."
      ]
    }
  ]
}